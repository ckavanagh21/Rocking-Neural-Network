{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-14T23:57:20.051203Z",
     "start_time": "2019-05-14T23:57:19.626384Z"
    }
   },
   "outputs": [],
   "source": [
    "import string\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare for Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-14T23:57:20.060842Z",
     "start_time": "2019-05-14T23:57:20.053165Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of file: 106526624\n",
      "All possible characters: 0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~ \t\n",
      "\u000b\f\n",
      "Number of all possible characters: 100\n"
     ]
    }
   ],
   "source": [
    "all_chars       = string.printable\n",
    "n_chars         = len(all_chars)\n",
    "file            = open('./trainlyrics.txt').read()\n",
    "file_len        = len(file)\n",
    "\n",
    "print('Length of file: {}'.format(file_len))\n",
    "print('All possible characters: {}'.format(all_chars))\n",
    "print('Number of all possible characters: {}'.format(n_chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-14T23:57:20.077596Z",
     "start_time": "2019-05-14T23:57:20.064808Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get a random sequence of rock lyrics.\n",
    "def get_random_seq():\n",
    "    seq_len     = 128  # The length of an input sequence.\n",
    "    start_index = random.randint(0, file_len - seq_len)\n",
    "    end_index   = start_index + seq_len + 1\n",
    "    return file[start_index:end_index]\n",
    "\n",
    "# Convert the sequence to one-hot tensor.\n",
    "def seq_to_onehot(seq):\n",
    "    tensor = torch.zeros(len(seq), 1, n_chars) \n",
    "    # Shape of the tensor:\n",
    "    #     (sequence length, batch size, classes)\n",
    "    # Here we use batch size = 1 and classes = number of unique characters.\n",
    "    for t, char in enumerate(seq):\n",
    "        index = all_chars.index(char)\n",
    "        tensor[t][0][index] = 1\n",
    "    return tensor\n",
    "\n",
    "# Convert the sequence to index tensor.\n",
    "def seq_to_index(seq):\n",
    "    tensor = torch.zeros(len(seq), 1)\n",
    "    # Shape of the tensor: \n",
    "    #     (sequence length, batch size).\n",
    "    # Here we use batch size = 1.\n",
    "    for t, char in enumerate(seq):\n",
    "        tensor[t] = all_chars.index(char)\n",
    "    return tensor\n",
    "\n",
    "# Sample a mini-batch including input tensor and target tensor.\n",
    "def get_input_and_target():\n",
    "    seq    = get_random_seq()\n",
    "    input  = seq_to_onehot(seq[:-1])      # Input is represented in one-hot.\n",
    "    target = seq_to_index(seq[1:]).long() # Target is represented in index.\n",
    "    return input, target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choose a Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-14T23:57:20.129301Z",
     "start_time": "2019-05-14T23:57:20.081156Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# If there are GPUs, choose the first one for computing. Otherwise use CPU.\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)  \n",
    "# If 'cuda:0' is printed, it means GPU is available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Network Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-14T23:57:22.437344Z",
     "start_time": "2019-05-14T23:57:20.131573Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (rnn): RNN(100, 100, num_layers=4, dropout=0.2)\n",
       "  (linear): Linear(in_features=100, out_features=100, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        # Initialization.\n",
    "        super(Net, self).__init__()\n",
    "        self.input_size  = n_chars   # Input size: Number of unique chars.\n",
    "        self.hidden_size = 100       # Hidden size: 100.\n",
    "        self.output_size = n_chars   # Output size: Number of unique chars.\n",
    "        self.num_layers = 4\n",
    "        \n",
    "        self.rnn = nn.RNN(self.input_size, self.hidden_size, num_layers=self.num_layers, dropout=0.2, nonlinearity=\"relu\")\n",
    "        self.linear = nn.Linear(self.hidden_size, self.output_size)\n",
    "    \n",
    "    def forward(self, input, hidden):\n",
    "        \"\"\" Forward function.\n",
    "              input:  One-hot input. It refers to the x_t in homework write-up.\n",
    "              hidden: Previous hidden state. It refers to the h_{t-1}.\n",
    "            Returns (output, hidden) where output refers to y_t and \n",
    "                     hidden refers to h_t.\n",
    "        \"\"\"\n",
    "        # Forward function.\n",
    "        output, hidden = self.rnn(input, hidden)\n",
    "        output = self.linear(output)\n",
    "\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self):\n",
    "        # Initial hidden state.\n",
    "        # 1 means batch size = 1.\n",
    "        return torch.zeros(self.num_layers, self.hidden_size).to(device) #made into a 2d tensor. \n",
    "    \n",
    "net = Net()     # Create the network instance.\n",
    "net.to(device)  # Move the network parameters to the specified device."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Step and Evaluation Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-14T23:57:22.449539Z",
     "start_time": "2019-05-14T23:57:22.440333Z"
    }
   },
   "outputs": [],
   "source": [
    "# Training step function.\n",
    "def train_step(net, opt, input, target):\n",
    "    \"\"\" Training step.\n",
    "        net:    The network instance.\n",
    "        opt:    The optimizer instance.\n",
    "        input:  Input tensor.  Shape: [seq_len, 1, n_chars].\n",
    "        target: Target tensor. Shape: [seq_len, 1].\n",
    "    \"\"\"\n",
    "    seq_len = input.shape[0]    # Get the sequence length of current input.\n",
    "    hidden = net.init_hidden()  # Initial hidden state.\n",
    "    net.zero_grad()             # Clear the gradient.\n",
    "    loss = 0                    # Initial loss.\n",
    "\n",
    "    for t in range(seq_len):    # For each one in the input sequence.\n",
    "        output, hidden = net(input[t], hidden)\n",
    "        loss += loss_func(output, target[t])\n",
    "\n",
    "    loss.backward()             # Backward. \n",
    "    opt.step()                  # Update the weights.\n",
    "\n",
    "    return loss / seq_len       # Return the average loss w.r.t sequence length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T03:10:36.378318Z",
     "start_time": "2019-05-15T03:10:36.366394Z"
    }
   },
   "outputs": [],
   "source": [
    "# Evaluation step function.\n",
    "def eval_step(net, init_seq='W', predicted_len=100):\n",
    "    # Initialize the hidden state, input and the predicted sequence.\n",
    "    hidden        = net.init_hidden()\n",
    "    init_input    = seq_to_onehot(init_seq).to(device)\n",
    "    predicted_seq = init_seq\n",
    "\n",
    "    # Use initial string to \"build up\" hidden state.\n",
    "    for t in range(len(init_seq) - 1):\n",
    "        output, hidden = net(init_input[t], hidden)\n",
    "        \n",
    "    # Set current input as the last character of the initial string.\n",
    "    input = init_input[-1]\n",
    "    \n",
    "    # Predict more characters after the initial string.\n",
    "    for t in range(predicted_len):\n",
    "        # Get the current output and hidden state.\n",
    "        output, hidden = net(input, hidden)\n",
    "        \n",
    "        # Sample from the output as a multinomial distribution.\n",
    "        predicted_index = torch.multinomial(output.view(-1).exp(), 1)[0]\n",
    "        \n",
    "        # Add predicted character to the sequence and use it as next input.\n",
    "        predicted_char  = all_chars[predicted_index]\n",
    "        predicted_seq  += predicted_char\n",
    "        \n",
    "        # Use the predicted character to generate the input of next round.\n",
    "        input = seq_to_onehot(predicted_char)[0].to(device)\n",
    "\n",
    "    return predicted_seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T00:38:13.556497Z",
     "start_time": "2019-05-14T23:57:22.478732Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter:999/50000 loss:2.742931842803955\n",
      "generated sequence: W waun Widd raeg hre to mowan Shaacs\n",
      "a s,s rokesa -\"wind on:\n",
      "Litiw, upart syiage her can'd Wten\n",
      "Tarln\n",
      "\n",
      "iter:1999/50000 loss:2.2511634826660156\n",
      "generated sequence: Wcruls the bohe hebild now some\n",
      "\n",
      "[Rurirps s,, sed\n",
      "I loves bnave,\n",
      "Neg indley\n",
      "To tive olk theus it farl\n",
      "\n",
      "iter:2999/50000 loss:2.1272690296173096\n",
      "generated sequence: Wt lest, and I eof's I'm nives saely atraus to pligkth\n",
      "But, rann voe ican\n",
      "\n",
      "\n",
      "You say to hert\n",
      "Youas is \n",
      "\n",
      "iter:3999/50000 loss:2.068357229232788\n",
      "generated sequence: W s), porke in's me a feveressperss becars\n",
      "I've weranning baus\n",
      "Buzes is chace hownraf the pay lorgh l\n",
      "\n",
      "iter:4999/50000 loss:2.0132298469543457\n",
      "generated sequence: Whaon buoking whome, louks be\n",
      "Of oh just I just to gu\n",
      "beut there\n",
      "bottide no'n' is want the ugutson al\n",
      "\n",
      "iter:5999/50000 loss:1.9599519968032837\n",
      "generated sequence: Wtet,\n",
      "Alwtey hond)\n",
      "Simesy seere't rick an and want\n",
      "She pinnre is's wathing only and kmpn drown Vnas-o\n",
      "\n",
      "iter:6999/50000 loss:1.9226635694503784\n",
      "generated sequence: When me she\n",
      "She trisin' you albuc,\n",
      "Your yois awuct therores Hog your sways\n",
      "Me old to cine-ow fire\n",
      "\n",
      "Bu\n",
      "\n",
      "iter:7999/50000 loss:1.919594407081604\n",
      "generated sequence: Way.\n",
      "\n",
      "You're world your lased the san\n",
      "If waited the sunary wupe out are to she mennf your wendith, i \n",
      "\n",
      "iter:8999/50000 loss:1.8685894012451172\n",
      "generated sequence: Withere more you but down'o kister\n",
      "Win I dever try of more\n",
      "I love tell, long I stle love You want's g\n",
      "\n",
      "iter:9999/50000 loss:1.8669519424438477\n",
      "generated sequence: Wkenome ciffmde be sun murblody we knew\n",
      "Sleaze thros\n",
      "\n",
      "I'm soon\n",
      "That wine enel) my longrass\n",
      "They's par\n",
      "\n",
      "iter:10999/50000 loss:1.8499839305877686\n",
      "generated sequence: W\n",
      "Go you eaghledll] You pasch an eyes up the meresing, love as I fistelt horth to look onebidi it out\n",
      "\n",
      "iter:11999/50000 loss:1.8234273195266724\n",
      "generated sequence: Went the wars\n",
      "I see my daryy tomodbydant marbaon fir\n",
      "It it's the like a right come doont\n",
      "foof of my s\n",
      "\n",
      "iter:12999/50000 loss:1.821087121963501\n",
      "generated sequence: Wouldw notcery't in my fanding my hungonechand\n",
      "Just unting, call the weome, will been a tike out when\n",
      "\n",
      "iter:13999/50000 loss:1.814181923866272\n",
      "generated sequence: Wo get a miste tereiss boind\n",
      "I'm denman herdy\n",
      "New lifpeney madior\n",
      "Hot thing, hay I was the hute\n",
      "man i\n",
      "\n",
      "iter:14999/50000 loss:1.7966655492782593\n",
      "generated sequence: Wawed relord \n",
      "Drowdes\n",
      "\n",
      "And\n",
      "And to crond the prabe if the ye for hod fry- of you. La,\n",
      "Celriciin\n",
      "so wel\n",
      "\n",
      "iter:15999/50000 loss:1.7610812187194824\n",
      "generated sequence: Ww\n",
      "i've phonnin'\"\n",
      "\"I deera.\"\n",
      "\"To the wait oh it?\n",
      "\n",
      "Sooning we care a chorny fampine to be thack and ba\n",
      "\n",
      "iter:16999/50000 loss:1.7882592678070068\n",
      "generated sequence: Whalable\n",
      "That's my time to things\n",
      "6L0ssent hemes of sleun, me, my licklecart to risting, hast a Hid.\"\n",
      "\n",
      "iter:17999/50000 loss:1.7918097972869873\n",
      "generated sequence: W me night\n",
      "Shake you kast too love my life to they sime tume\n",
      "Thing you seem me beliald wime\n",
      "What vulc\n",
      "\n",
      "iter:18999/50000 loss:1.7670069932937622\n",
      "generated sequence: Ways\n",
      "We triening a my dhiet time that's all my hand what you've but mind can\n",
      "Any juy! I kniwn of cual\n",
      "\n",
      "iter:19999/50000 loss:1.759731411933899\n",
      "generated sequence: W'm werpevel the endsore's ecay, in the rane, where the\n",
      "way withdow as stirk with beses-lanneds\n",
      "and t\n",
      "\n",
      "iter:20999/50000 loss:1.7551361322402954\n",
      "generated sequence: Whocarer, and moon nownoins\n",
      "That's 'nother woll\n",
      "Last are\n",
      "In Myself i can askoomay\n",
      "Of I die of a sased\n",
      "\n",
      "iter:21999/50000 loss:1.765113353729248\n",
      "generated sequence: We her kyou, Way\n",
      "You've been foar I doen\n",
      "Thy she l vove than way)\n",
      "and were mave\n",
      "And that do\n",
      "Reoly\n",
      "You\n",
      "\n",
      "iter:22999/50000 loss:1.7587532997131348\n",
      "generated sequence: Wulle fare, it don't want of think days You have is I head and aindy plhy me it heart on it it just e\n",
      "\n",
      "iter:23999/50000 loss:1.7423920631408691\n",
      "generated sequence: Whpue out all out graa\n",
      "Black with, aCaundle wtrt for gotta\n",
      "all be the sagreds would whel Jake freeloc\n",
      "\n",
      "iter:24999/50000 loss:1.764747977256775\n",
      "generated sequence: Wwrive\n",
      "I gotit\n",
      "You Are you ligtle to neblyyw Pr\n",
      "id me I can't everyouerin'\n",
      "\n",
      "To the sead you heller sw\n",
      "\n",
      "iter:25999/50000 loss:1.7444429397583008\n",
      "generated sequence: Warkad fron Wind\n",
      "The that Roar\n",
      "I'll fuyes be of your thies that with to believe on girlose it one thr\n",
      "\n",
      "iter:26999/50000 loss:1.7239704132080078\n",
      "generated sequence: Wtak sue\n",
      "Time\n",
      "And it will he bust sole?.\n",
      "\n",
      "You mose me swared\n",
      "Kome light she so wrave by sunturean you\n",
      "\n",
      "iter:27999/50000 loss:1.7448525428771973\n",
      "generated sequence: Wwent falling nevory be wolise, Rie if sondorcloquowans gonnan we only know I let mohed and,\n",
      "(Wild th\n",
      "\n",
      "iter:28999/50000 loss:1.728258728981018\n",
      "generated sequence: Why ain't you guopped is say cur You back be\n",
      "We's could man's don't please)\n",
      "And judcer fall -\n",
      "Mister \n",
      "\n",
      "iter:29999/50000 loss:1.7113759517669678\n",
      "generated sequence: W mabe bleamlige lost and one\"\n",
      "\"I bull with\n",
      "On to mear, If the down and come Hurs us over string they\n",
      "\n",
      "iter:30999/50000 loss:1.7141584157943726\n",
      "generated sequence: When someo. tharghle, we itkout her pil is you\n",
      "You've bect you thas am the for the gabicglowe har it\n",
      "\n",
      "\n",
      "iter:31999/50000 loss:1.7184747457504272\n",
      "generated sequence: We's dead all whet I'd you, shiptul she's use't fun\n",
      "I can you're dhiess my necror onfe\n",
      "Overry for nob\n",
      "\n",
      "iter:32999/50000 loss:1.714138150215149\n",
      "generated sequence: W\n",
      "Wulling's cring\n",
      "From like of the pried \n",
      "He's sick..\n",
      "Lade do you said Love\n",
      "may like chear\n",
      "It'd lonel\n",
      "\n",
      "iter:33999/50000 loss:1.7212916612625122\n",
      "generated sequence: Wiint men to me time, roans his asker\n",
      "\n",
      "Blased\n",
      "Ooh, kis fire\n",
      "awout as all don't was \"\n",
      "\"\"\"You're, yeah \n",
      "\n",
      "iter:34999/50000 loss:1.715842366218567\n",
      "generated sequence: Wtten to were in the hand on the not up bangeccing bloter stapte you girl\n",
      "sity\n",
      "men\n",
      "and I cubk that\n",
      "Th\n",
      "\n",
      "iter:35999/50000 loss:1.703177809715271\n",
      "generated sequence: Wusade was homes, there's aprid,\n",
      "Came On\n",
      "Sayte lipe wifhem na cleaby my her morive sicfalm pellion ou\n",
      "\n",
      "iter:36999/50000 loss:1.712096095085144\n",
      "generated sequence: When a sing?\n",
      "Shated, !\n",
      "It ruin I never beying\n",
      "I'm games wish his don't hat \n",
      "Ondown\n",
      "Yeah of! oving old\n",
      "\n",
      "iter:37999/50000 loss:1.7020314931869507\n",
      "generated sequence: W \n",
      "How me\n",
      "I'm no sund frimt wait it's now its me you hope arout everything\n",
      "in the rove\n",
      "welln\n",
      "todibad \n",
      "\n",
      "iter:38999/50000 loss:1.7159446477890015\n",
      "generated sequence: Whoinn. feel intmy, an pofaw then ighn myself the leans in \n",
      "\n",
      "St down in a never goin' song mainca fri\n",
      "\n",
      "iter:39999/50000 loss:1.707623839378357\n",
      "generated sequence: Wh\n",
      "A teal\n",
      "In my be womry sweet you are) I'm wonder va Hardils, sarm before\n",
      "Stoned that shick\n",
      "Barge to\n",
      "\n",
      "iter:40999/50000 loss:1.698616623878479\n",
      "generated sequence: Went was turn\n",
      "Well it filleds planey.\n",
      "They watch\n",
      "I love was Hols of a Hweresietdays has pither kented\n",
      "\n",
      "iter:41999/50000 loss:1.6942849159240723\n",
      "generated sequence: Wtar, by dream? \n",
      "And all sarey, crose charle's forever neviss/- the conper.\"\"\n",
      "\n",
      "Have feel aid mimines \n",
      "\n",
      "iter:42999/50000 loss:1.694614052772522\n",
      "generated sequence: Who?\n",
      "\n",
      "I was promice)\n",
      "\"\"'ne tume my fill shouth theis Chuend doesbince was day\n",
      "But I'm buckin' time, \n",
      "\n",
      "\n",
      "iter:43999/50000 loss:1.6981050968170166\n",
      "generated sequence: Wook of the night other nerd\n",
      "she starpine bear\n",
      "\"\"Sey wight I don't saighenven' lack miting russing me\n",
      "\n",
      "iter:44999/50000 loss:1.6924513578414917\n",
      "generated sequence: What sand an bleesinascease her the otcesuli\n",
      "Tme for the Maco fere\n",
      "You go aul fine to hear then somet\n",
      "\n",
      "iter:45999/50000 loss:1.693050742149353\n",
      "generated sequence: We made; \n",
      "He's home only you as in.\n",
      "Ribe the pless - Tatts a rugn\n",
      "Me it me worlth\n",
      "Left to le\n",
      "we'll ta\n",
      "\n",
      "iter:46999/50000 loss:1.691998839378357\n",
      "generated sequence: When you in behind you take to don't never die do it's a wand me the are you meed her new I know doff\n",
      "\n",
      "iter:47999/50000 loss:1.6974493265151978\n",
      "generated sequence: What nive encery to yes\n",
      "'hes's helly ofh, ax\n",
      "Thatafk, aclam-lEt fear on trather\n",
      "Yeah if you know I do\n",
      "\n",
      "iter:48999/50000 loss:1.6910061836242676\n",
      "generated sequence: Where I think what you be,\"\"\n",
      "You draving to \n",
      "She's under\n",
      "We never you streoon\n",
      "When a come,\n",
      "\n",
      "But the b\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Number of iterations.\n",
    "# NOTE: You may reduce the number of training iterations if the training takes long.\n",
    "iters       = 50000  # Number of training iterations.\n",
    "print_iters = 1000    # Number of iterations for each log printing.\n",
    "\n",
    "# The loss variables.\n",
    "all_losses = []\n",
    "loss_sum   = 0\n",
    "\n",
    "# Initialize the optimizer and the loss function.\n",
    "opt       = torch.optim.Adam(net.parameters(), lr=0.001) \n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training procedure.\n",
    "for i in range(iters):\n",
    "    input, target = get_input_and_target()            # Fetch input and target.\n",
    "    input, target = input.to(device), target.to(device) # Move to GPU memory.\n",
    "    loss      = train_step(net, opt, input, target)   # Calculate the loss.\n",
    "    loss_sum += loss                                  # Accumulate the loss.\n",
    "\n",
    "    # Print the log.\n",
    "    if i % print_iters == print_iters - 1:\n",
    "        print('iter:{}/{} loss:{}'.format(i, iters, loss_sum / print_iters))\n",
    "        print('generated sequence: {}\\n'.format(eval_step(net)))\n",
    "              \n",
    "        # Track the loss.\n",
    "        all_losses.append(loss_sum / print_iters)\n",
    "        loss_sum = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stopped the program at 11899 iterations, could not get to less than 3.1 loss. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Loss Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T00:38:13.728474Z",
     "start_time": "2019-05-15T00:38:13.559531Z"
    }
   },
   "outputs": [],
   "source": [
    "items = [ele.item() for ele in all_losses] \n",
    "plt.xlabel('iters')\n",
    "plt.ylabel('loss')\n",
    "plt.plot(items)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation: A Sample of Generated Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T03:10:52.267837Z",
     "start_time": "2019-05-15T03:10:51.986701Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(eval_step(net, predicted_len=600))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
